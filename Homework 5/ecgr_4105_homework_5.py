# -*- coding: utf-8 -*-
"""ECGR 4105 Homework 5

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1SVU7l17NBVDKNnykFbKIjgZvNford6bp
"""

import numpy as np
import pandas as pd
import torch
import torch.nn as nn
import torch.optim as optim
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import StandardScaler

# Load dataset
housing_url = "https://raw.githubusercontent.com/HamedTabkhi/Intro-to-ML/refs/heads/main/Dataset/Housing.csv"
housing_df = pd.read_csv(housing_url)

# DATASET PROCESSING (Part 1)
#converts yes/no columns to binary
binary_columns = ['mainroad', 'guestroom', 'basement', 'hotwaterheating', 'airconditioning', 'prefarea']
housing_df[binary_columns] = housing_df[binary_columns].replace({'yes': 1, 'no': 0})
housing_df.pop('furnishingstatus')

# Split features and target
housing_X = housing_df.drop('price', axis=1)
housing_Y = housing_df['price']

# SCALING THE DATASET
scaler = StandardScaler()
housing_X = scaler.fit_transform(housing_X)

print(housing_X[:5])

# DATASET PROCESSING (Part 2)
housing_X = torch.tensor(housing_X, dtype=torch.float32)
housing_Y = torch.tensor(housing_Y.values, dtype=torch.float32).view(-1, 1)  # Ensure housing_Y is shaped correctly (N, 1)

#splits the dataset
housing_train_X, housing_test_X, housing_train_Y, housing_test_Y = train_test_split(housing_X, housing_Y, test_size=0.2, random_state=42)

#gets the indices of the desired columns for problem 2
desired_columns = ['area', 'bedrooms', 'bathrooms', 'stories', 'parking']
column_indices = [housing_df.columns.get_loc(col) for col in desired_columns]

# Select the desired columns from the NumPy arrays
small_housing_train_X = pd.DataFrame(housing_train_X[:, column_indices], columns=desired_columns)
small_housing_test_X = pd.DataFrame(housing_test_X[:, column_indices], columns=desired_columns)

#all columns for problem 3 - Assuming you want all 11 columns here
full_housing_train_X = pd.DataFrame(housing_train_X, columns=housing_df.columns[:-1]) # Exclude 'price' column
full_housing_test_X = pd.DataFrame(housing_test_X, columns=housing_df.columns[:-1]) # Exclude 'price' column

# Define the linear regression model using nn.Linear
small_input_dim = small_housing_train_X.shape[1]
small_model = nn.Linear(small_input_dim, 1)

# Define the loss function and optimizer
loss_function = nn.MSELoss()

epochs = 5000

# TRAINING LOOP WITH LEARNING RATE OF 0.1
learning_rate = 0.1
small_optimizer = optim.Adam(small_model.parameters(), lr=learning_rate)

# TRAINING LOOP
epochs = 5000

for epoch in range(epochs + 1):
    # Forward pass
    small_housing_Y_pred = small_model(torch.tensor(small_housing_train_X.values, dtype=torch.float32))

    # Compute loss
    small_loss = loss_function(small_housing_Y_pred, housing_train_Y)

    # Zero gradients, perform a backward pass, and update the weights
    small_optimizer.zero_grad()  # Zero the gradients
    small_loss.backward()        # Backpropagate the loss
    small_optimizer.step()       # Update the parameters

    # Optionally, print loss for monitoring
    if epoch % 500 == 0:
        print(f"Epoch {epoch} \tLoss: {small_loss.item()}")

# TRAINING LOOP WITH LEARNING RATE OF 0.01
learning_rate = 0.01
small_optimizer = optim.Adam(small_model.parameters(), lr=learning_rate)

# TRAINING LOOP
epochs = 5000

for epoch in range(epochs + 1):
    # Forward pass
    small_housing_Y_pred = small_model(torch.tensor(small_housing_train_X.values, dtype=torch.float32))

    # Compute loss
    small_loss = loss_function(small_housing_Y_pred, housing_train_Y)

    # Zero gradients, perform a backward pass, and update the weights
    small_optimizer.zero_grad()  # Zero the gradients
    small_loss.backward()        # Backpropagate the loss
    small_optimizer.step()       # Update the parameters

    # Optionally, print loss for monitoring
    if epoch % 500 == 0:
        print(f"Epoch {epoch} \tLoss: {small_loss.item()}")

# TRAINING LOOP WITH LEARNING RATE OF 0.001
learning_rate = 0.001
small_optimizer = optim.Adam(small_model.parameters(), lr=learning_rate)

# TRAINING LOOP
epochs = 5000

for epoch in range(epochs + 1):
    # Forward pass
    small_housing_Y_pred = small_model(torch.tensor(small_housing_train_X.values, dtype=torch.float32))

    # Compute loss
    small_loss = loss_function(small_housing_Y_pred, housing_train_Y)

    # Zero gradients, perform a backward pass, and update the weights
    small_optimizer.zero_grad()  # Zero the gradients
    small_loss.backward()        # Backpropagate the loss
    small_optimizer.step()       # Update the parameters

    # Optionally, print loss for monitoring
    if epoch % 500 == 0:
        print(f"Epoch {epoch} \tLoss: {small_loss.item()}")

# TRAINING LOOP WITH LEARNING RATE OF 0.0001
learning_rate = 0.0001
small_optimizer = optim.Adam(small_model.parameters(), lr=learning_rate)

# TRAINING LOOP
epochs = 5000

for epoch in range(epochs + 1):
    # Forward pass
    small_housing_Y_pred = small_model(torch.tensor(small_housing_train_X.values, dtype=torch.float32))

    # Compute loss
    small_loss = loss_function(small_housing_Y_pred, housing_train_Y)

    # Zero gradients, perform a backward pass, and update the weights
    small_optimizer.zero_grad()  # Zero the gradients
    small_loss.backward()        # Backpropagate the loss
    small_optimizer.step()       # Update the parameters

    # Optionally, print loss for monitoring
    if epoch % 500 == 0:
        print(f"Epoch {epoch} \tLoss: {small_loss.item()}")

"""I'll be completely honest, I have no idea why these losses are so high. At least these are consistently high, so the issue might be something else than the training loop. If I would have to guess, it has something to do with the data processing."""

full_input_dim = full_housing_train_X.shape[1]
full_model = nn.Linear(full_input_dim, 1)

# TRAINING LOOP WITH LEARNING RATE OF 0.1
learning_rate = 0.1
optimizer = optim.Adam(model.parameters(), lr=learning_rate)

# TRAINING LOOP
epochs = 5000

for epoch in range(epochs + 1):
    # Forward pass: compute predicted y by passing x to the model
    housing_Y_pred = full_model(housing_train_X)

    # Compute loss
    loss = loss_function(housing_Y_pred, housing_train_Y)

    # Zero gradients, perform a backward pass, and update the weights
    optimizer.zero_grad()  # Zero the gradients
    loss.backward()        # Backpropagate the loss
    optimizer.step()       # Update the parameters

    # Optionally, print loss for monitoring
    if epoch % 500 == 0:
        print(f"Epoch {epoch} \tLoss: {loss.item()}")

# TESTING PHASE (Optional)
with torch.no_grad():  # Disable gradient calculation for testing
    housing_Y_test_pred = model(housing_test_X)
    test_loss = loss_function(housing_Y_test_pred, housing_test_Y)
    print(f"Test\t\tLoss: {test_loss.item()}")

# TRAINING LOOP WITH LEARNING RATE OF 0.01
learning_rate = 0.01
optimizer = optim.Adam(model.parameters(), lr=learning_rate)

# TRAINING LOOP
epochs = 5000

for epoch in range(epochs + 1):
    # Forward pass: compute predicted y by passing x to the model
    housing_Y_pred = full_model(housing_train_X)

    # Compute loss
    loss = loss_function(housing_Y_pred, housing_train_Y)

    # Zero gradients, perform a backward pass, and update the weights
    optimizer.zero_grad()  # Zero the gradients
    loss.backward()        # Backpropagate the loss
    optimizer.step()       # Update the parameters

    # Optionally, print loss for monitoring
    if epoch % 500 == 0:
        print(f"Epoch {epoch} \tLoss: {loss.item()}")

# TESTING PHASE (Optional)
with torch.no_grad():  # Disable gradient calculation for testing
    housing_Y_test_pred = model(housing_test_X)
    test_loss = loss_function(housing_Y_test_pred, housing_test_Y)
    print(f"Test\t\tLoss: {test_loss.item()}")

# TRAINING LOOP WITH LEARNING RATE OF 0.001
learning_rate = 0.001
optimizer = optim.Adam(model.parameters(), lr=learning_rate)

# TRAINING LOOP
epochs = 5000

for epoch in range(epochs + 1):
    # Forward pass: compute predicted y by passing x to the model
    housing_Y_pred = full_model(housing_train_X)

    # Compute loss
    loss = loss_function(housing_Y_pred, housing_train_Y)

    # Zero gradients, perform a backward pass, and update the weights
    optimizer.zero_grad()  # Zero the gradients
    loss.backward()        # Backpropagate the loss
    optimizer.step()       # Update the parameters

    # Optionally, print loss for monitoring
    if epoch % 500 == 0:
        print(f"Epoch {epoch} \tLoss: {loss.item()}")

# TESTING PHASE (Optional)
with torch.no_grad():  # Disable gradient calculation for testing
    housing_Y_test_pred = model(housing_test_X)
    test_loss = loss_function(housing_Y_test_pred, housing_test_Y)
    print(f"Test\t\tLoss: {test_loss.item()}")

# TRAINING LOOP WITH LEARNING RATE OF 0.0001
learning_rate = 0.0001
optimizer = optim.Adam(model.parameters(), lr=learning_rate)

# TRAINING LOOP
epochs = 5000

for epoch in range(epochs + 1):
    # Forward pass: compute predicted y by passing x to the model
    housing_Y_pred = full_model(housing_train_X)

    # Compute loss
    loss = loss_function(housing_Y_pred, housing_train_Y)

    # Zero gradients, perform a backward pass, and update the weights
    optimizer.zero_grad()  # Zero the gradients
    loss.backward()        # Backpropagate the loss
    optimizer.step()       # Update the parameters

    # Optionally, print loss for monitoring
    if epoch % 500 == 0:
        print(f"Epoch {epoch} \tLoss: {loss.item()}")

# TESTING PHASE (Optional)
with torch.no_grad():  # Disable gradient calculation for testing
    housing_Y_test_pred = model(housing_test_X)
    test_loss = loss_function(housing_Y_test_pred, housing_test_Y)
    print(f"Test\t\tLoss: {test_loss.item()}")